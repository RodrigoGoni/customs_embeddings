{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Custom embedddings con Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset canciones de bandas de habla inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l7z4CSBfpR3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-07 13:14:03--  http://songs_dataset.zip/\n",
      "Resolving songs_dataset.zip (songs_dataset.zip)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘songs_dataset.zip’\n",
      "--2025-11-07 13:14:04--  https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
      "Resolving github.com (github.com)... 4.228.31.150\n",
      "Connecting to github.com (github.com)|4.228.31.150|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip [following]\n",
      "--2025-11-07 13:14:04--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2075036 (2,0M) [application/zip]\n",
      "Saving to: ‘songs_dataset.zip’\n",
      "\n",
      "songs_dataset.zip   100%[===================>]   1,98M   738KB/s    in 2,7s    \n",
      "\n",
      "2025-11-07 13:14:08 (738 KB/s) - ‘songs_dataset.zip’ saved [2075036/2075036]\n",
      "\n",
      "FINISHED --2025-11-07 13:14:08--\n",
      "Total wall clock time: 4,2s\n",
      "Downloaded: 1 files, 2,0M in 2,7s (738 KB/s)\n"
     ]
    }
   ],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "import os\n",
    "import platform\n",
    "if os.access('./songs_dataset', os.F_OK) is False:\n",
    "    if os.access('songs_dataset.zip', os.F_OK) is False:\n",
    "        if platform.system() == 'Windows':\n",
    "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
    "        else:\n",
    "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
    "    !unzip -q songs_dataset.zip   \n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mysGrIw9ljC2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul-simon.txt',\n",
       " 'leonard-cohen.txt',\n",
       " 'jimi-hendrix.txt',\n",
       " 'beatles.txt',\n",
       " 'lin-manuel-miranda.txt',\n",
       " 'lorde.txt',\n",
       " 'adele.txt',\n",
       " 'joni-mitchell.txt',\n",
       " 'missy-elliott.txt',\n",
       " 'lil-wayne.txt',\n",
       " 'bob-dylan.txt',\n",
       " 'disney.txt',\n",
       " 'nirvana.txt',\n",
       " 'nursery_rhymes.txt',\n",
       " 'dickinson.txt',\n",
       " 'dr-seuss.txt',\n",
       " 'alicia-keys.txt',\n",
       " 'blink-182.txt',\n",
       " 'patti-smith.txt',\n",
       " 'amy-winehouse.txt',\n",
       " 'radiohead.txt',\n",
       " 'bruce-springsteen.txt',\n",
       " 'bjork.txt',\n",
       " 'lady-gaga.txt',\n",
       " 'dolly-parton.txt',\n",
       " 'rihanna.txt',\n",
       " 'michael-jackson.txt',\n",
       " 'prince.txt',\n",
       " 'al-green.txt',\n",
       " 'bieber.txt',\n",
       " 'cake.txt',\n",
       " 'ludacris.txt',\n",
       " 'notorious_big.txt',\n",
       " 'janisjoplin.txt',\n",
       " 'dj-khaled.txt',\n",
       " 'kanye-west.txt',\n",
       " 'johnny-cash.txt',\n",
       " 'eminem.txt',\n",
       " 'kanye.txt',\n",
       " 'nicki-minaj.txt',\n",
       " 'Lil_Wayne.txt',\n",
       " 'bruno-mars.txt',\n",
       " 'Kanye_West.txt',\n",
       " 'notorious-big.txt',\n",
       " 'nickelback.txt',\n",
       " 'britney-spears.txt',\n",
       " 'drake.txt',\n",
       " 'r-kelly.txt',\n",
       " 'bob-marley.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posibles bandas\n",
    "os.listdir(\"./songs_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160475/4107314280.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('songs_dataset/lady-gaga.txt', sep='/n', header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'll undress you, 'cause you're tired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cover you as you desire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you fall asleep inside my arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May not have the fancy things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But I'll give you everything</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "0  I'll undress you, 'cause you're tired\n",
       "1                Cover you as you desire\n",
       "2    When you fall asleep inside my arms\n",
       "3          May not have the fancy things\n",
       "4           But I'll give you everything"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
    "df = pd.read_csv('songs_dataset/lady-gaga.txt', sep='/n', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 3807\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "### 1 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 13:18:13.467006: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-07 13:18:13.530559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-07 13:18:14.675803: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
    "for _, row in df[:None].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"i'll\", 'undress', 'you', \"'cause\", \"you're\", 'tired'],\n",
       " ['cover', 'you', 'as', 'you', 'desire']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "### 2 - Crear los vectores (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "i0wnDdv9sJ47"
   },
   "outputs": [],
   "source": [
    "# Crearmos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5lTt8wErsf17"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model.build_vocab(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TNc9qt4os5AT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 3807\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "idw9cHF3tSMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 722\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9mZ8DPk-UC"
   },
   "source": [
    "### 3 - Entrenar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QSp-x0PAsq56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 225482.796875\n",
      "Loss after epoch 1: 155035.328125\n",
      "Loss after epoch 2: 152820.625\n",
      "Loss after epoch 3: 148431.625\n",
      "Loss after epoch 4: 138253.5\n",
      "Loss after epoch 5: 131583.375\n",
      "Loss after epoch 6: 122535.75\n",
      "Loss after epoch 7: 108953.25\n",
      "Loss after epoch 8: 104393.75\n",
      "Loss after epoch 9: 101013.25\n",
      "Loss after epoch 10: 98637.125\n",
      "Loss after epoch 11: 96883.375\n",
      "Loss after epoch 12: 95152.0\n",
      "Loss after epoch 13: 93183.25\n",
      "Loss after epoch 14: 92545.125\n",
      "Loss after epoch 15: 91719.625\n",
      "Loss after epoch 16: 91131.625\n",
      "Loss after epoch 17: 87097.625\n",
      "Loss after epoch 18: 82691.0\n",
      "Loss after epoch 19: 81990.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(372010, 606000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model.train(sentence_tokens,\n",
    "                 total_examples=w2v_model.corpus_count,\n",
    "                 epochs=20,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tap', 0.9334839582443237),\n",
       " ('bow', 0.9194691181182861),\n",
       " ('goes', 0.8999143242835999),\n",
       " ('street', 0.8603769540786743),\n",
       " ('those', 0.8600791096687317),\n",
       " ('chase', 0.8560516238212585),\n",
       " ('lonely', 0.848296582698822),\n",
       " ('listen', 0.826325535774231),\n",
       " ('burn', 0.8195632696151733),\n",
       " ('bring', 0.8118451237678528)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"pray\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "47HiU5gdkdMq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', -0.1458824872970581),\n",
       " ('his', -0.157462939620018),\n",
       " ('to', -0.16939155757427216),\n",
       " ('alejandro', -0.17621156573295593),\n",
       " (\"she's\", -0.17670799791812897),\n",
       " ('heart', -0.17988987267017365),\n",
       " ('floor', -0.18076960742473602),\n",
       " ('nobody', -0.186057910323143),\n",
       " ('make', -0.18935710191726685),\n",
       " ('so', -0.1907348930835724)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "w2v_model.wv.most_similar(negative=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DT4Rvno2mD65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fangs', 0.9204478859901428),\n",
       " ('teeth', 0.8739274740219116),\n",
       " ('open', 0.8721127510070801),\n",
       " ('show', 0.8357691764831543),\n",
       " ('mouth', 0.8266028165817261),\n",
       " ('quit', 0.7868215441703796),\n",
       " ('end', 0.7769829630851746),\n",
       " ('round', 0.7709757089614868),\n",
       " ('truth', 0.768057107925415),\n",
       " ('lovers', 0.7626691460609436)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"lord\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XPLDPgzBmQXt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fancy', 0.8555483818054199),\n",
       " ('aha', 0.8468345999717712),\n",
       " ('eh', 0.8340397477149963),\n",
       " (\"that's\", 0.8336365222930908),\n",
       " ('faster', 0.8172706961631775)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"money\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_UvHPMMklOr"
   },
   "outputs": [],
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "w2v_model.wv.most_similar(negative=[\"diedaa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02308867  0.4989994  -0.09772125  0.3247849   0.17555384  0.07140884\n",
      "  0.4374889   0.12166818 -0.2540025  -0.11188604  0.1843571  -0.00847309\n",
      "  0.10333195  0.04072281 -0.28295925  0.01112818  0.01088119  0.27480528\n",
      " -0.00107298 -0.1688643  -0.11177701 -0.17242876  0.13007943  0.42336705\n",
      " -0.15853404  0.02897894 -0.17196481 -0.13828687  0.05504058 -0.04482167\n",
      "  0.03885011 -0.19878817  0.13278128  0.2271028  -0.22426909  0.26837105\n",
      "  0.14764781 -0.1059263  -0.24700642  0.11171305 -0.20681983 -0.12497331\n",
      "  0.16483329  0.22778313  0.273591    0.09938694 -0.02638428 -0.11866252\n",
      "  0.05156144  0.37531793 -0.04071515  0.01639819  0.04541741  0.09187078\n",
      " -0.05626888 -0.11509284 -0.43191615  0.06140081  0.02493665 -0.10181837\n",
      " -0.03297804 -0.28322178  0.1128493  -0.21662079  0.00569549 -0.24158387\n",
      "  0.36527023  0.07453392 -0.02212254 -0.20098059  0.0801177   0.21510217\n",
      "  0.04860479  0.5812557   0.07561997  0.06853098 -0.03368496 -0.21928751\n",
      "  0.2944462   0.3886212  -0.2769008  -0.16999595 -0.2009221  -0.11517988\n",
      "  0.31155762 -0.2691025  -0.04369209  0.10194987  0.1850693  -0.03706356\n",
      " -0.44284943  0.35110214  0.14972451  0.23140062  0.13963526  0.00664483\n",
      "  0.10031091  0.02458815 -0.12648162  0.11524941 -0.16621342 -0.14724208\n",
      "  0.11895125 -0.01293356 -0.12988527 -0.17398073 -0.10799817  0.2704425\n",
      "  0.02753506 -0.03034474 -0.0749664  -0.17214577  0.04913163 -0.22707333\n",
      "  0.03848356  0.12939107  0.22515899  0.06600101  0.23009151  0.06749169\n",
      "  0.09240128  0.09961797  0.14492993 -0.06489764 -0.02631997  0.38496086\n",
      " -0.07216984 -0.2292059  -0.25487262 -0.05090983  0.02244602  0.1224407\n",
      "  0.24094264 -0.31612036 -0.2745325  -0.27483502 -0.48535573 -0.40174085\n",
      " -0.11609293 -0.08096667  0.18552656 -0.13723512  0.03320021  0.12504525\n",
      "  0.32386842 -0.1537438  -0.24831207 -0.10712748 -0.2582322  -0.02061261\n",
      "  0.21987832  0.1529855  -0.15096381  0.01659802 -0.5038634  -0.05321097\n",
      " -0.26257828 -0.0383316  -0.26619738  0.02730596 -0.00410117  0.24754828\n",
      " -0.03274    -0.02565402 -0.01212662  0.33554375 -0.17175518 -0.05163214\n",
      " -0.32244566  0.05432017 -0.1848547   0.1682711  -0.18746157  0.1717695\n",
      "  0.23789622  0.2130795  -0.32078686 -0.19186486 -0.08851885  0.1863966\n",
      " -0.1890131  -0.00332767 -0.3393556  -0.03293741 -0.22370617 -0.04599396\n",
      "  0.14208405  0.01085124 -0.04003139 -0.33299124 -0.0425928   0.26379585\n",
      " -0.22450136 -0.0039422   0.16884187 -0.0277875   0.29486984 -0.16970967\n",
      "  0.02488842 -0.17811482 -0.17058764 -0.09362361 -0.0184754   0.1391971\n",
      " -0.05903774  0.10561527 -0.08608688 -0.09851038 -0.02425741 -0.01847271\n",
      "  0.27758634 -0.12408946 -0.13758256  0.01421567  0.12327418 -0.32937723\n",
      " -0.24483965  0.17208049 -0.15448485  0.08781268  0.12350006 -0.38240784\n",
      "  0.05425912 -0.14463547  0.12832102 -0.03952414  0.10014743 -0.14818579\n",
      "  0.05322332  0.13163571  0.1301522  -0.10099191 -0.03341733 -0.02685866\n",
      "  0.16122323  0.39481482  0.10985752  0.0261528   0.11562694 -0.04092884\n",
      "  0.04375423  0.03628677  0.3854313  -0.13403422 -0.09279002 -0.49270582\n",
      "  0.24168754  0.0274503   0.0452912  -0.05441796  0.09736934 -0.07255975\n",
      "  0.39260727  0.16203713 -0.37847796 -0.18353167 -0.05691316  0.1392011\n",
      " -0.17945194 -0.25790808  0.12608875 -0.0434786   0.12982154 -0.072751\n",
      " -0.46830052 -0.17220297  0.17031075 -0.0102917  -0.2599882   0.09996656\n",
      " -0.00629118 -0.21071985  0.23926857 -0.05947785  0.03540862  0.26450777\n",
      " -0.0276089   0.2435863  -0.06224438  0.18565086 -0.08059021 -0.45058465\n",
      "  0.19137095 -0.03584595 -0.00623833 -0.13078748 -0.2613078  -0.11730341\n",
      " -0.13863061  0.13693362  0.00337915 -0.01580797 -0.15232351  0.24213746\n",
      " -0.10716584  0.37401333 -0.31519583 -0.20223768  0.17321056  0.03531636]\n"
     ]
    }
   ],
   "source": [
    "# el método `get_vector` permite obtener los vectores:\n",
    "vector_love = w2v_model.wv.get_vector(\"love\")\n",
    "print(vector_love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.9999999403953552),\n",
       " (\"doin'\", 0.7452163696289062),\n",
       " ('game', 0.7287333607673645),\n",
       " ('play', 0.6739518642425537),\n",
       " (\"wasn't\", 0.6699321866035461),\n",
       " ('denim', 0.6677326560020447),\n",
       " ('fame', 0.6619865894317627),\n",
       " ('start', 0.6596845388412476),\n",
       " ('yourself', 0.6507260203361511),\n",
       " ('whoa', 0.6472256779670715)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# el método `most_similar` también permite comparar a partir de vectores\n",
    "w2v_model.wv.most_similar(vector_love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"doin'\", 0.7452163696289062),\n",
       " ('game', 0.7287333011627197),\n",
       " ('play', 0.6739518642425537),\n",
       " (\"wasn't\", 0.6699321269989014),\n",
       " ('denim', 0.6677326560020447),\n",
       " ('fame', 0.6619865894317627),\n",
       " ('start', 0.6596844792366028),\n",
       " ('yourself', 0.6507260203361511),\n",
       " ('whoa', 0.6472256779670715),\n",
       " ('song', 0.6442990303039551)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### 5 - Visualizar agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m MAX_WORDS=\u001b[32m200\u001b[39m\n\u001b[32m      8\u001b[39m fig = px.scatter(x=vecs[:MAX_WORDS,\u001b[32m0\u001b[39m], y=vecs[:MAX_WORDS,\u001b[32m1\u001b[39m], text=labels[:MAX_WORDS])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# para VS Code Jupyter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Maestria/Materias/procesamiento_lenguaje_natural/.venv/lib/python3.12/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Maestria/Materias/procesamiento_lenguaje_natural/.venv/lib/python3.12/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "# Usar matplotlib como alternativa si plotly tiene problemas\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vecs[:MAX_WORDS,0], vecs[:MAX_WORDS,1], alpha=0.5)\n",
    "for i, label in enumerate(labels[:MAX_WORDS]):\n",
    "    plt.annotate(label, (vecs[i,0], vecs[i,1]), fontsize=8, alpha=0.7)\n",
    "plt.title('Word Embeddings 2D Visualization')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 3D\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model,3)\n",
    "\n",
    "# Usar matplotlib para 3D visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(vecs[:MAX_WORDS,0], vecs[:MAX_WORDS,1], vecs[:MAX_WORDS,2], alpha=0.5)\n",
    "ax.set_title('Word Embeddings 3D Visualization')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
    "# http://projector.tensorflow.org/\n",
    "\n",
    "\n",
    "vectors = np.asarray(w2v_model.wv.vectors)\n",
    "labels = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
    "\n",
    "with open(\"labels.tsv\", \"w\") as fp:\n",
    "    for item in labels:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMM_SHSaZ9N-"
   },
   "source": [
    "### Alumno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WivQZ3ZCZ9N_"
   },
   "source": [
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "- Graficarlos.\n",
    "- Obtener conclusiones."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
